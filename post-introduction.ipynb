{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early desaturation alarm system using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post presents some promising results of an early alarm model for pulsioximeter data using Deep Learning.\n",
    "\n",
    "First of all, let's describe the purpose of such an early alarm predictive model: cetecting that a desaturation is likely to happen in the next 2 minutes. (A desaturation means a low blood oxygen concentration). Working in the model is in part a feasibility exercise, although the prediction would be useful on itself, at least as an initial step, for a system that improves the sleep quality of sleep of the patient (e.g. if the desaturation alarm was raised early enough, the carer of the patient could just change the position of the patient -assuming a patient who is sleeping and has very limited ability to move).\n",
    "\n",
    "On the other hand, importantly for the purposes of this blog, it is a nice practical case of implementation of a Deep Learning model; the task at hand is certainly cumbersome, however there is a number of steps to follow that are pretty much common knowledge for the Deep Learning community, and after some effort, quite a lot of CPU time, and a few tricks, the results are quite satisfying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has already been presented at [this previous post](http://sisifospage.tech/2017-05-15-time-series-clustering-pulsi.html).\n",
    "\n",
    "It consists of data from a pulsioximeter (i.e. a device for measuring bood oxygen concentration and pulse rate). The meter was connected to a sick patient for 39 nights, plus 2 additional nights that were recorded for a healthy patient, as a sanity check.\n",
    "\n",
    "The capturing itself was done using software in [this repository](https://github.com/Iukekini/Baby-Monitor-Masimo-Pulse-Oximeter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's so many references for Deep Learning. [This](https://www.business-science.io/timeseries-analysis/2018/04/18/keras-lstm-sunspots-time-series-prediction.html) and [this](https://www.business-science.io/timeseries-analysis/2018/07/01/keras-lstm-sunspots-part2.html) are two parts of a quite a good tutorial for predicting a time series with LSTMs; may be a gentle introduction, also because the problem described in the tutorial is quite a good example of a time series to be predicted that can be reasonably dealt with the use of Deep Learning.\n",
    "\n",
    "There are around many other examples of exercises that are indeed good for practising the technique where the forecasting problem is however too difficult, such as predicting a time series for the stock exchange market. Those exercises usually try a prediction for just 1 timestep; although the prediction might be decent enough, it all becomes a lot harder when adding more timesteps (like in a real problem). Anyhow, a really useful example for this kind is [this post](http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/), which tries to predict long term interest rates for the USA. \n",
    "\n",
    "There has been quite a few attempts to use Deep Learning to predict time series related to body signals in similar kind of prediction setups; the implementation in this post is actually a refactor of [this repository](https://github.com/NLeSC/mcfly), called McFly, for Human Activity Recognition; the repository is an implementation of [this paper](https://www.mdpi.com/1424-8220/16/1/115/htm), co-written by [F Ordoñez](https://www.youtube.com/watch?v=7gsIkXpZx9E) -the youtube talk is in Spanish, sorry.\n",
    "\n",
    "The refactored code used in this post is available [here](https://github.com/lrnzcig/sisifoDL), and mostly adds a `Data Generator` to [McFly](https://github.com/NLeSC/mcfly), for setting up the data more easily and, importantly, for controlling how data is fed to training the model.\n",
    "\n",
    "If these in the list are still not enough, you could backup to couple of golden references on DL such as [Chollet's bible](https://www.manning.com/books/deep-learning-with-python) or [Andrew Ng's course](https://www.coursera.org/specializations/deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the more relevant results (together with the code to reproduce them) are presented in this post, with the intention of keeping it brief (i.e. less long) and useful. In further follow-up posts, other details will be covered in more depth, so that at the end, hopefully, the set of articles would be a summary on the steps to build up a Deep Learning model for a time series prediction.\n",
    "\n",
    "Assuming that the right architecture and hyperparameters have been found already, in each of the following paragraphs the data is fed during training of the model in different ways:\n",
    "- Using data as it is (naïve approach)\n",
    "- Rebalancing data, since it is by far more interesting to detect big desaturations, rather than spurious variations of the\n",
    "oxygen levels\n",
    "- Augmenting the data, i.e. producing more data from the already existing data by means of simple variations\n",
    "\n",
    "Finally the post incluides a brief wrap-up on what has been done, what is missing, and what will be covered in follow-up posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning results for the most naïve approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, data is fed to the model _\"as is\"_. The data is generated in [this previous post](http://sisifospage.tech/2017-05-15-time-series-clustering-pulsi.html), anyhow for the purposes here one can just use [this csv file](http://sisifospage.tech/data/42nights.csv). The `csv` has 4 columns:\n",
    "* datetime\n",
    "* bpm: heart beat per minute\n",
    "* spo2: saturation level\n",
    "* name: string to identify each of the time series which correspond to a certain patient in a certain night\n",
    "\n",
    "Note that the raw data from the pulsioximeter comes out every second (every 2 seconds for other devices); the data in the `csv` has been interpolated to have one timestep every 30 seconds, as a pre-processing step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume at this point that the right architecture has already been selected somehow. First step for training the model is importing library and custom utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T14:16:39.175957Z",
     "start_time": "2019-10-29T14:16:16.062530Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.losses import mean_absolute_percentage_error\n",
    "\n",
    "from utils.generate_models import generate_models, generate_DeepConvLSTM_model\n",
    "from utils.validate_models import find_best_architecture, evaluate_model, evaluate_plot\n",
    "from utils.data_generator import DataGenerator\n",
    "from utils.get_dataset_pulsi import get_dataset_pulsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and pre-processing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T14:16:39.841182Z",
     "start_time": "2019-10-29T14:16:39.179944Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = np.array(['bpm', 'spo2'])\n",
    "dataset_reduced_std, dataset_reduced = get_dataset_pulsi(columns,\n",
    "                                                         filename='./utils/test_data/42nights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up some parameters of the model: the target prediction is for 4 timesteps, i.e. 2 minutes, based on the data of the las 12 timesteps, i.e. the last 6 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T14:16:39.853153Z",
     "start_time": "2019-10-29T14:16:39.846170Z"
    }
   },
   "outputs": [],
   "source": [
    "window_size = 12\n",
    "number_of_predictions = 4\n",
    "target_variable = \"spo2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some technical parameters: `batch_size` is the number of samples taken for each training step (where a sample is composed of the window of 12 timesteps plus the prediction of 4 timesteps) and the metric used for the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T14:16:39.878085Z",
     "start_time": "2019-10-29T14:16:39.873097Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "metric = mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the job is done by the `DataGenerator`'s, which control how the data is passed to the training process. More in-depth details on data generators will be given in follow-up posts; for the moment, it is important to note that they become very handy when one needs to control how the data is passed to the training process. For the generators instantiated below, the data is passed \"as is\", in batches of `batch_size`.\n",
    "\n",
    "Some nights are reserved for the training data, others for validation (used during training), and a final test set is reserved for assessing the precision of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T14:27:16.644791Z",
     "start_time": "2019-10-29T14:16:39.881077Z"
    }
   },
   "outputs": [],
   "source": [
    "train_names = np.array(['p_17-01-19', 'p_17-01-20', 'p_17-01-21', 'p_17-01-22', 'p_17-01-23', 'p_17-01-24', 'p_17-01-25',\n",
    "                        'p_17-01-26', 'p_17-01-27', 'p_17-01-28', 'p_17-01-29', 'p_17-01-30', 'p_17-01-31', 'p_17-02-01',\n",
    "                        'p_17-02-02', 'p_17-02-03', 'p_17-02-04', 'p_17-02-05', 'p_17-02-06', 'p_17-02-07', 'p_17-02-08',\n",
    "                        'p_17-02-09', 'p_17-02-10'])\n",
    "val_names = np.array(['p_17-02-11', 'p_17-02-12', 'p_17-02-13', 'p_17-02-14', 'p_17-02-15', 'p_17-02-16', 'p_17-02-17', 'p_17-02-18'])\n",
    "test_names = np.array(['p_17-02-19', 'p_17-02-20', 'p_17-02-21', 'p_17-02-22', 'p_17-02-23', 'p_17-02-24', 'p_17-02-25', 'p_17-04-27'])\n",
    "train_gen = DataGenerator(dataset_reduced_std, train_names,\n",
    "                          \"spo2\", batch_size=batch_size,\n",
    "                          number_of_predictions=number_of_predictions,\n",
    "                          window_size=window_size,\n",
    "                          step_prediction_dates=1,\n",
    "                          rebalance_data=False, debug=False)\n",
    "val_gen = DataGenerator(dataset_reduced_std, val_names,\n",
    "                        \"spo2\", batch_size=batch_size,\n",
    "                        number_of_predictions=number_of_predictions,\n",
    "                        window_size=window_size,\n",
    "                        step_prediction_dates=1,\n",
    "                        rebalance_data=False)\n",
    "test_gen = DataGenerator(dataset_reduced_std, test_names,\n",
    "                         \"spo2\", batch_size=batch_size,\n",
    "                         number_of_predictions=number_of_predictions,\n",
    "                         window_size=window_size,\n",
    "                         step_prediction_dates=1,\n",
    "                         rebalance_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step fits the model.\n",
    "\n",
    "Note that, first, a pre-fixed set of values of hyperparameters id defined. Then, the model is instantiated. And finally, the call to `find_best_architecture` trains the model and checks its peformance.\n",
    "\n",
    "All these would actually be done for different values of the model hyperparameters, for finding the best architecture -once again will, the subject will be covered in more depth in follow-up posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T14:27:18.760146Z",
     "start_time": "2019-10-29T14:27:16.647784Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e8a9273ff769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m                                     \u001b[0mregularization_rate_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                                     \u001b[0mdropout_rnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout_rnn_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_cnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout_cnn_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                                     metrics=[mean_absolute_percentage_error])\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mmodels_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyperparameters_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\github\\sisifoDL\\utils\\generate_models.py\u001b[0m in \u001b[0;36mgenerate_DeepConvLSTM_model\u001b[1;34m(dim_length, dim_channels, output_dim, filters, lstm_dims, learning_rate, regularization_rate, metrics, dropout, dropout_rnn, dropout_cnn, alternative_out, set_seed, clipvalue)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mweightinit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lecun_uniform'\u001b[0m  \u001b[1;31m# weight initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initialize model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# reshape a 2 dimensional array per window/variables into a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\sisifoDL\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\sisifoDL\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\sisifoDL\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;31m# Subclassed network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_subclassed_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_base_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\sisifoDL\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_init_subclassed_network\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_subclassed_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expects_training_arg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\sisifoDL\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_base_init\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\sisifoDL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[1;34m(prefix)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "hyperparameters_losses = {}\n",
    "regularization_rate_losses = 0.0666\n",
    "hyperparameters_losses['regularization_rate'] = regularization_rate_losses\n",
    "learning_rate_losses = 0.0006\n",
    "hyperparameters_losses['learning_rate'] = learning_rate_losses\n",
    "filters_losses = [78]\n",
    "hyperparameters_losses['filters'] = filters_losses\n",
    "lstm_dims_losses = [100]\n",
    "hyperparameters_losses['lstm_dims'] = lstm_dims_losses\n",
    "\n",
    "dropout_rnn_losses = 0.74\n",
    "dropout_cnn_losses = 0.27\n",
    "\n",
    "nrepochs_losses = 96\n",
    "\n",
    "dim_length = window_size\n",
    "dim_channels = 2         # spo2 and bpm\n",
    "output_dim = number_of_predictions\n",
    "\n",
    "model = generate_DeepConvLSTM_model(dim_length, dim_channels, output_dim,\n",
    "                                    filters_losses, lstm_dims_losses, learning_rate_losses,\n",
    "                                    regularization_rate_losses, dropout=None,\n",
    "                                    dropout_rnn=dropout_rnn_losses, dropout_cnn=dropout_cnn_losses,\n",
    "                                    metrics=[mean_absolute_percentage_error])\n",
    "models_losses = [(model, hyperparameters_losses)]\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "best_model_losses, best_params_losses, best_model_metrics, best_params_metrics, debug = \\\n",
    "    find_best_architecture(train_gen, val_gen, test_gen,\n",
    "                           verbose=False, number_of_models=None, nr_epochs=500, # let early stopping decide\n",
    "                           early_stopping=True, batch_size=batch_size,\n",
    "                           models=models_losses, metric=mean_absolute_percentage_error, use_testset=True,\n",
    "                           debug=False, test_retrain=False, output_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
