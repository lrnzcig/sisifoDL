post 1
======

# Pulsioximeter early alarm system using Deep Learning

## Introduction & purpose

This post presents some promising results of an early alarm model for pulsioximeter data using Deep Learning.

What's the purpose of the early alarm model? Detecting that a desaturation is likely to happen in the next 2 minutes.
(A desaturation means a low blood oxygen concentration). The output of such a model is useful on itself, at least as an initial
step for a system that improves the sleep quality of sleep of the patient (e.g. if the desaturation alarm was 
raised early enough, the carer of the patient could just change the position of the patient -assuming a patient that 
is sleeping and has very limited ability to move).

On the other hand, importantly for the purposes of this blog, it is a nice practical case of implementation of a Deep
Learning model; the task at hand is certainly cumbersome, however there is a number of steps to follow that are pretty much
common knowledge for the Deep Learning community, and after some effort, quite a lot of CPU time, and a few tricks, the
results are quite satisfying.

### Dataset used

The dataset has already been presented at [this previous post](http://sisifospage.tech/2017-05-15-time-series-clustering-pulsi.html).

It consists of data from a pulsioximeter (i.e. a device for measuring bood oxygen concentration and pulse rate). The meter was
connected to a sick patient for 39 nights, plus 2 additional nights that were recorded for a healthy patient, as a sanity check.

The capturing itself was done using software in [this repository](https://github.com/Iukekini/Baby-Monitor-Masimo-Pulse-Oximeter).

### References

There's so many references for Deep Learning. [This](https://www.business-science.io/timeseries-analysis/2018/04/18/keras-lstm-sunspots-time-series-prediction.html) 
and [this](https://www.business-science.io/timeseries-analysis/2018/07/01/keras-lstm-sunspots-part2.html)
are two parts of a quite a good tutorial for predicting a time series with LSTMs; may be a introduction. The problem 
described in the this tutorial is quite a good example since it can be reasonably dealt with the use of Deep Learning. There are
many other examples of exercises that are indeed good for practising but in a very difficult setup, such as predicting a time
series for the stock exchange market. Those exercises usually try a prediction for just 1 timestep, and the prediction might be
decent enough, but it all becomes a lot harder when adding more timesteps. A useful example for this kind of post is 
[this one](http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/), which by the way predicts long term
interest rates for the USA. 

There has been quite a few attempts to use Deep Learning to predict signals in similar kind of prediction setups; the
implementation in this post is actually a refactor of [this repository](https://github.com/NLeSC/mcfly) for Human Activity
Recognition, which is an implementation of [this paper](https://www.mdpi.com/1424-8220/16/1/115/htm), co-written by 
[F Ordoñez](https://www.youtube.com/watch?v=7gsIkXpZx9E) -the talk is in Spanish, sorry.

The refactored code is available [here](https://github.com/lrnzcig/sisifoDL), and mostly adds a `Data Generator` for setting up
the data more easily and, importantly, for controlling how data is fed to training the model.

If these are still not enough, you could backup to couple of references such as [Chollet's bible](https://www.manning.com/books/deep-learning-with-python) 
or [Ng's reference course](https://www.coursera.org/specializations/deep-learning)

## Summary of results

In this post, with the intention of keeping it brief (i.e. less long) and useful, only the most important results are presented
(together with the code to reproduce them). In follow-up posts, other details will be covered in more depth, so that at the end,
hopefully, the set of post would be a good summary on the steps to build up a Deep Learning model for a time series prediction.

In the following paragraphs it is assumed that the right architecture and hyperparameters have been found already, but during
training of the model the data is fed in different ways:
- Using data as it is (naïve approach)
- Rebalancing data, since it is by far more interesting to detect big desaturations, rather than spurious variations of the
oxygen levels
- Augmenting the data, i.e. producing more data from the already existing data by means of simple variations

To concluide the post, at the bottom, a brief wrap-up on what has been done, what is missing, and what will be covered in
following posts.

### Deep Learning results for the most naïve approach

In this case, data is fed to the model "as is". 

xxx

The results are not very useful though. Even for a well-tuned architecture, output predictions are not really useful, even if
they are more or less accurate most of the time. Most of the time the patient is more or less ok, the series shows small 
variations, and it does not really matter if the prediction follows those variations. However there is a few times during the
night in which the patient desatures, the oxygen level slopes down abruptly, and those are the situations to predict for the
early alarm system. 

### Deep Learning results when rebalancing data



### Deep Learning results when augmenting data

## Next steps for the model and follow-up posts




Pasos principales:
Preparar el generador de datos
Pruebas iniciales (ajuste sin variables explicativas, baseline de comparación, etc)
Búsqueda de arquitectura óptima con búsqueda de hiperparámetros “aleatoria” (ejecutar en GPU en la nube, las pruebas pueden costar horas o días y requieren memoria)
Aumentar gradualmente la complejidad de la red (1º solo RNN, después también con CNN, e ir añadiendo capas)


Conclusiones: cuándo usarlo
Con pocos datos de entrenamiento, último recurso cuando otras metodologías han fallado.
Cuando las variables explicativas tienen poco poder predictivo. Comprobar primero un ajuste sin variables explicativas (el resultado puede ser sorprendente)
Si ok, añadir variables explicativas y buscar hiperparámetros de modelo óptimo 
¿Merece la pena, en comparación con la posibilidad de incorporar nuevas variables?

Con muchos datos de entrenamiento, mejora la expectativa de obtener un buen resultado, sobre todo si la serie a predecir tiene autocorrelación alta.



